{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data import (\n",
    "    H4Tokenizer, \n",
    "    LMDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from lib.model import (\n",
    "    CausalMask,\n",
    "    PadMask,\n",
    "    PositionalEncoding,\n",
    "    DecoderOnlyTransformer\n",
    ")\n",
    "from lib.utils import (\n",
    "    create_optimizer,\n",
    "    create_scheduler,\n",
    "    plot_lr_schedule\n",
    ")\n",
    "from lib.trainers import (\n",
    "    LMTrainer,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import shutil\n",
    "import wandb\n",
    "import yaml\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV2GWmKd7Q65"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLChmBx67Q65"
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Name\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"char\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:                  \n",
    "  root                 : \"data/p1_data\"\n",
    "  train_partition      : \"train\" \n",
    "  val_partition        : \"val\"    \n",
    "  test_partition       : \"test\"   \n",
    "  subset               : 1.0     \n",
    "  batch_size           : 256    \n",
    "  NUM_WORKERS          : 2       \n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model:\n",
    "  d_model                   : 256\n",
    "  d_ff                      : 1024\n",
    "  num_layers                : 2\n",
    "  num_heads                 : 2\n",
    "  dropout                   : 0.0\n",
    "  layer_drop_rate           : 0.0\n",
    "  weight_tying              : False\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : False  \n",
    "  wandb_run_id                : \"none\"  \n",
    "  resume                      : False \n",
    "  epochs                      : 20\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"Set-Project-Name-Here\"  \n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss:  \n",
    "  label_smoothing: 0.0\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adam\"  \n",
    "  lr: 5.0e-4    \n",
    "\n",
    "  weight_decay: 0.0001\n",
    "\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []   \n",
    "      lr: 0.0001     \n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "    - name: ffn\n",
    "      patterns: []  \n",
    "      lr: 0.0001  \n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"   \n",
    "\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  \n",
    "    factor: 0.1  \n",
    "    patience: 10  \n",
    "    threshold: 0.0001   \n",
    "    threshold_mode: \"rel\"  \n",
    "    cooldown: 0   \n",
    "    min_lr: 0.0000001  \n",
    "    eps: 1.0e-8  \n",
    "\n",
    "  cosine:\n",
    "    T_max: 15  \n",
    "    eta_min: 1.0e-8  \n",
    "    last_epoch: -1\n",
    "\n",
    "  cosine_warm:\n",
    "    T_0: 4  \n",
    "    T_mult: 4  \n",
    "    eta_min: 0.0000001  \n",
    "    last_epoch: -1\n",
    "\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  \n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl_9Vv117Q66"
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu56OILL7Q66"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBrysj6-7Q66"
   },
   "outputs": [],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'],\n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-vsGdfu7Q66"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCR3fGoL7Q66"
   },
   "outputs": [],
   "source": [
    "train_dataset  = LMDataset(\n",
    "    partition  = config['data']['train_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "val_dataset    = LMDataset(\n",
    "    partition  = config['data']['val_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "test_dataset   = LMDataset(\n",
    "    partition  = config['data']['test_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf8Y_COP7Q66"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GajQ0LX7Q66"
   },
   "outputs": [],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn\n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCrlXlZH7Q67"
   },
   "source": [
    "## Calculate Max Transcript Length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWAWHucJ7Q67"
   },
   "outputs": [],
   "source": [
    "max_transcript_length = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Global Max Transcript Length':<30} : {max_transcript_length}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot4OuRE27Q67"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL2-8IbL7Q67"
   },
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "model_config.update({\n",
    "    'max_len': max_transcript_length,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "model = DecoderOnlyTransformer(**model_config)\n",
    "\n",
    "for batch in train_loader:\n",
    "    shifted_transcripts, golden_transcripts, transcript_lengths = batch\n",
    "    print(\"Shape of shifted_transcripts : \", shifted_transcripts.shape)\n",
    "    print(\"Shape of golden_transcripts  : \", golden_transcripts.shape)\n",
    "    print(\"Shape of transcript_lengths  : \", transcript_lengths.shape)\n",
    "    break\n",
    "\n",
    "model_stats = summary(model, input_data=[shifted_transcripts, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH973f3x7Q67"
   },
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQAemeOM7Q68"
   },
   "source": [
    "## Trainer\n",
    "\n",
    "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
    "```\n",
    "expts/\n",
    "    \u2514\u2500\u2500 {run_name}/\n",
    "        \u251c\u2500\u2500 config.yaml\n",
    "        \u251c\u2500\u2500 model_arch.txt\n",
    "        \u251c\u2500\u2500 checkpoints/\n",
    "        \u2502   \u251c\u2500\u2500 checkpoint-best-metric-model.pth\n",
    "        \u2502   \u2514\u2500\u2500 checkpoint-last-epoch-model.pth\n",
    "        \u251c\u2500\u2500 attn/\n",
    "        \u2502   \u2514\u2500\u2500 {attention visualizations}\n",
    "        \u2514\u2500\u2500 text/\n",
    "            \u2514\u2500\u2500 {generated text outputs}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcvGSnWi7Q68"
   },
   "outputs": [],
   "source": [
    "trainer = LMTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"test-lm\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJREaFhqiPrT"
   },
   "source": [
    "### Setup Optimizer and Scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WStK_vNzzQRq"
   },
   "source": [
    "#### Setting up the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkljGtIPkATt"
   },
   "outputs": [],
   "source": [
    "trainer.optimizer = create_optimizer(\n",
    "    model=model,\n",
    "    opt_config=config['optimizer']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AxafmUnzQRq"
   },
   "source": [
    "#### Setting up the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXrwTbqdiPE_"
   },
   "outputs": [],
   "source": [
    "trainer.scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XI0dHJB7Q68"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nugAKoOw7Q68"
   },
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=config['training']['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j55r9gK_7Q68"
   },
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72D0yzHr7Q68"
   },
   "outputs": [],
   "source": [
    "test_metrics, test_generation_results = trainer.evaluate(test_loader)\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qQG51p6e7Q6x"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}